{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pejmanrasti/From_Shallow_to_Deep/blob/main/02_end_to_end_machine_learning_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpZsFhovpEiU"
      },
      "source": [
        "**End-to-end Machine Learning project**\n",
        "\n",
        "*Welcome to Machine Learning Housing Corp.! Your task is to predict median house values in Californian districts, given a number of features from these districts.*\n",
        "\n",
        "© Aurélien Géron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slW6CEqSpEiV"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFp-xnZbpEiV"
      },
      "source": [
        "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiSy8ZZjpEiV"
      },
      "outputs": [],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"end_to_end_project\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YILC3qaApEiV"
      },
      "source": [
        "# Get the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPrFE0O3pEiV"
      },
      "source": [
        "## Download the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYteneiipEiW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
        "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
        "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
        "\n",
        "#function to fetch the data:\n",
        "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
        "    if not os.path.isdir(housing_path):\n",
        "        os.makedirs(housing_path)\n",
        "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
        "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
        "    housing_tgz = tarfile.open(tgz_path)\n",
        "    housing_tgz.extractall(path=housing_path)\n",
        "    housing_tgz.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EkX8AkspEiW"
      },
      "outputs": [],
      "source": [
        "#when you call fetch_housing_data(), it creates a datasets/housing directory in\n",
        "#your workspace, downloads the housing.tgz file, and extracts the housing.csv from it in\n",
        "#this directory.\n",
        "fetch_housing_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DsobD7MpEiW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "\n",
        "#function to load the data using Pandas\n",
        "def load_housing_data(housing_path=HOUSING_PATH):\n",
        "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
        "    return pd.read_csv(csv_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL31JyCRpEiW"
      },
      "source": [
        "## Take a Quick Look at the Data Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYLySrN_pEiW"
      },
      "outputs": [],
      "source": [
        "housing = load_housing_data() #This function returns a Pandas DataFrame object containing all the data.\n",
        "housing.head() #look at the top five rows using the DataFrame’s head() method \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDGuEXVfpEiW"
      },
      "outputs": [],
      "source": [
        " #The info() method is useful to get a quick description of the data, in particular the\n",
        "#total number of rows, and each attribute’s type and number of non-null values\n",
        "housing.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGljvlkbpEiX"
      },
      "outputs": [],
      "source": [
        "#You can find out what categories exist and how many districts belong to each category by using the\n",
        "#value_counts() method:\n",
        "housing[\"ocean_proximity\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "japh0VI4pEiX"
      },
      "outputs": [],
      "source": [
        "#The describe() method shows a summary of the numerical attributes\n",
        "housing.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "________________________\n",
        "\n",
        "Another quick way to get a feel of the type of data you are dealing with is to plot a histogram for each numerical attribute. "
      ],
      "metadata": {
        "id": "lvNE136gzoB4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVRkK_l1pEiX"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "housing.hist(bins=50, figsize=(20,15))\n",
        "save_fig(\"attribute_histogram_plots\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice a few things in these histograms:\n",
        "\n",
        "First, the median income attribute has been scaled and capped at The housing median age and the median house value were also capped. \n",
        "\n",
        "Working with preprocessed attributes is common in Machine Learning,and it is not necessarily a problem, but you should try to understand how the data was computed. The median house value may be a serious problem since it is your target attribute (your labels). Your Machine Learning algorithms may learn that prices never go beyond that limit.\n",
        "\n",
        "\n",
        "These attributes have very different scales. We will discuss this later in this chapter when we explore feature scaling.\n",
        "\n",
        "\n",
        "Finally, many histograms are tail heavy: they extend much farther to the right of the median than to the left. This may make it a bit harder for some Machine Learning algorithms to detect patterns. We will try transforming these attributes\n",
        "later on to have more bell-shaped distributions.\n",
        "________________________________\n"
      ],
      "metadata": {
        "id": "CdHC3drn0Y3m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbDqrkXIpEiX"
      },
      "source": [
        "Wait! Before you look at the data any further, you need to create a\n",
        "test set, put it aside, and never look at it.\n",
        "\n",
        "\n",
        "## Create a Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4m9njeWVpEiY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpqEFWS1pEiY"
      },
      "outputs": [],
      "source": [
        "test_set.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose you chatted with experts who told you that the median income is a very\n",
        "important attribute to predict median housing prices. You may want to ensure that\n",
        "the test set is representative of the various categories of incomes in the whole dataset.\n",
        "Since the median income is a continuous numerical attribute, you first need to create\n",
        "an income category attribute. Let’s look at the median income histogram more closely"
      ],
      "metadata": {
        "id": "x6sxJmYV5uvF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEN5U77_pEiZ"
      },
      "outputs": [],
      "source": [
        "housing[\"median_income\"].hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "most median income values are clustered around 1.5 to 6 but some median incomes go far beyond 6. It is important to have\n",
        "a sufficient number of instances in your dataset for each stratum, or else the estimate\n",
        "of the stratum’s importance may be biased. This means that you should not have too\n",
        "many strata, and each stratum should be large enough."
      ],
      "metadata": {
        "id": "uvmxPObF55Qv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaH2NoI1pEiZ"
      },
      "outputs": [],
      "source": [
        "## uses the pd.cut() function to create an income category attribute with 5 categories\n",
        "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
        "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
        "                               labels=[1, 2, 3, 4, 5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iNPOMx2pEiZ"
      },
      "outputs": [],
      "source": [
        "housing[\"income_cat\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g9BumAspEiZ"
      },
      "outputs": [],
      "source": [
        "housing[\"income_cat\"].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QZmZk1fpEiZ"
      },
      "outputs": [],
      "source": [
        "# Now you are ready to do stratified sampling based on the income category.\n",
        "## Stratified sampling: the population is divided into homogeneous subgroups called strata,\n",
        "## and the right number of instances is sampled from each stratum to guarantee that the\n",
        "## test set is representative of the overall population.\n",
        "\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
        "    strat_train_set = housing.loc[train_index]\n",
        "    strat_test_set = housing.loc[test_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvwldXy-pEiZ"
      },
      "outputs": [],
      "source": [
        "# looking at the income category proportions in the test set:\n",
        "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV_ksk2TpEiZ"
      },
      "outputs": [],
      "source": [
        "# looking at the income category proportions in the whole data set:\n",
        "housing[\"income_cat\"].value_counts() / len(housing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn8kfxcIpEiZ"
      },
      "outputs": [],
      "source": [
        "# remove the income_cat attribute so the data is back to its original state:\n",
        "for set_ in (strat_train_set, strat_test_set):\n",
        "    set_.drop(\"income_cat\", axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaEmVGPypEiZ"
      },
      "source": [
        "# Discover and Visualize the Data to Gain Insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7x0wZpecpEiZ"
      },
      "outputs": [],
      "source": [
        "#create a copy so you can play with it without harming the training set:\n",
        "housing = strat_train_set.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYAAcV2NpEiZ"
      },
      "source": [
        "## Visualizing Geographical Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34JfXDcPpEiZ"
      },
      "outputs": [],
      "source": [
        "# Since there is geographical information (latitude and longitude), it is a good idea to\n",
        "#create a scatterplot of all districts to visualize the data \n",
        "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n",
        "save_fig(\"bad_visualization_plot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSUUIYU5pEia"
      },
      "outputs": [],
      "source": [
        "# This looks like California all right, but other than that it is hard to see any particular pattern. \n",
        "# Setting the alpha option to 0.1 makes it much easier to visualize the places\n",
        "# where there is a high density of data points \n",
        "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\n",
        "save_fig(\"better_visualization_plot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9WPgRxUpEia"
      },
      "source": [
        "The argument `sharex=False` fixes a display bug (the x-axis values and legend were not displayed). This is a temporary fix (see: https://github.com/pandas-dev/pandas/issues/10611 ). Thanks to Wilmer Arellano for pointing it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNcOUfWSpEia"
      },
      "outputs": [],
      "source": [
        "# The radius of each circle represents the district’s population (option s), \n",
        "# and the color represents the price (option c). \n",
        "# We will use a predefined color map (option cmap) called jet,\n",
        "# which ranges from blue (low values) to red (high prices):\n",
        "\n",
        "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
        "             s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
        "             c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
        "             sharex=False)\n",
        "plt.legend()\n",
        "save_fig(\"housing_prices_scatterplot\")\n",
        "# This image tells you that the housing prices are very much related to the location (e.g., close to the ocean) and to the population density, as you probably knew already."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHgEsHBNpEia"
      },
      "outputs": [],
      "source": [
        "# Download the California image\n",
        "images_path = os.path.join(PROJECT_ROOT_DIR, \"images\", \"end_to_end_project\")\n",
        "os.makedirs(images_path, exist_ok=True)\n",
        "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
        "filename = \"california.png\"\n",
        "print(\"Downloading\", filename)\n",
        "url = DOWNLOAD_ROOT + \"images/end_to_end_project/\" + filename\n",
        "urllib.request.urlretrieve(url, os.path.join(images_path, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEIEQaLWpEia"
      },
      "outputs": [],
      "source": [
        "import matplotlib.image as mpimg\n",
        "california_img=mpimg.imread(os.path.join(images_path, filename))\n",
        "ax = housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7),\n",
        "                  s=housing['population']/100, label=\"Population\",\n",
        "                  c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n",
        "                  colorbar=False, alpha=0.4)\n",
        "plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,\n",
        "           cmap=plt.get_cmap(\"jet\"))\n",
        "plt.ylabel(\"Latitude\", fontsize=14)\n",
        "plt.xlabel(\"Longitude\", fontsize=14)\n",
        "\n",
        "prices = housing[\"median_house_value\"]\n",
        "tick_values = np.linspace(prices.min(), prices.max(), 11)\n",
        "cbar = plt.colorbar(ticks=tick_values/prices.max())\n",
        "cbar.ax.set_yticklabels([\"$%dk\"%(round(v/1000)) for v in tick_values], fontsize=14)\n",
        "cbar.set_label('Median House Value', fontsize=16)\n",
        "\n",
        "plt.legend(fontsize=16)\n",
        "save_fig(\"california_housing_prices_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flIo-9hgpEia"
      },
      "source": [
        "## Looking for Correlations\n",
        "\n",
        "Since the dataset is not too large, you can easily compute the standard correlation\n",
        "coecient (also called Pearson’s r) between every pair of attributes using the corr() method.\n",
        "\n",
        "The correlation coefficient ranges from –1 to 1. When it is close to 1, it means that\n",
        "there is a strong positive correlation; for example, the median house value tends to go\n",
        "up when the median income goes up. When the coefficient is close to –1, it means that there is a strong negative correlation; Finally, coefficients close to zero mean that there is no\n",
        "linear correlation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1eXoH_QpEia"
      },
      "outputs": [],
      "source": [
        "corr_matrix = housing.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soQff_WepEia"
      },
      "outputs": [],
      "source": [
        "# Now let’s look at how much each attribute correlates with the median house value:\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhHHlR3XpEia"
      },
      "outputs": [],
      "source": [
        "# Another way to check for correlation between attributes is to use Pandas’\n",
        "# scatter_matrix function, which plots every numerical attribute against every other numerical attribute.\n",
        "# from pandas.tools.plotting import scatter_matrix # For older versions of Pandas\n",
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
        "              \"housing_median_age\"]\n",
        "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
        "save_fig(\"scatter_matrix_plot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqC6RhuapEia"
      },
      "outputs": [],
      "source": [
        "# The most promising attribute to predict the median house value is the median income, \n",
        "# so let’s zoom in on their correlation scatterplot\n",
        "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n",
        "             alpha=0.1)\n",
        "plt.axis([0, 16, 0, 550000])\n",
        "save_fig(\"income_vs_house_value_scatterplot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot reveals a few things. \n",
        "\n",
        "First, the correlation is indeed very strong; you can\n",
        "clearly see the upward trend and the points are not too dispersed. \n",
        "\n",
        "Second, the price cap that we noticed earlier is clearly visible as a horizontal line at 500,000. But this\n",
        "plot reveals other less obvious straight lines: a horizontal line around 450,000,\n",
        "another around 350,000.\n",
        "\n",
        "You may want to try removing the corresponding districts to prevent your algorithms\n",
        "from learning to reproduce these data quirks."
      ],
      "metadata": {
        "id": "OKiKb5zx_8cv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DcevXBJpEib"
      },
      "source": [
        "# Prepare the Data for Machine Learning Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4YgE7RspEib"
      },
      "outputs": [],
      "source": [
        "\n",
        "#seprate labels\n",
        "housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
        "housing_labels = strat_train_set[\"median_house_value\"].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4IlmubApEib"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyL_b08PpEib"
      },
      "source": [
        "Most Machine Learning algorithms cannot work with missing features, so let’s create\n",
        "a few functions to take care of them. \n",
        "\n",
        "• Get rid of the corresponding districts.\n",
        "\n",
        "• Get rid of the whole attribute.\n",
        "\n",
        "• Set the values to some value (zero, the mean, the median, etc.).\n",
        "\n",
        "\n",
        "```python\n",
        "housing.dropna(subset=[\"total_bedrooms\"])    # option 1\n",
        "housing.drop(\"total_bedrooms\", axis=1)       # option 2\n",
        "median = housing[\"total_bedrooms\"].median()  # option 3\n",
        "housing[\"total_bedrooms\"].fillna(median, inplace=True)\n",
        "```\n",
        "\n",
        "To demonstrate each of them, let's create a copy of the housing dataset, but keeping only the rows that contain at least one null. Then it will be easier to visualize exactly what each option does:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84zgaJvxpEib"
      },
      "outputs": [],
      "source": [
        "sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\n",
        "sample_incomplete_rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oT2Oc2TOpEib"
      },
      "outputs": [],
      "source": [
        "sample_incomplete_rows.dropna(subset=[\"total_bedrooms\"])    # option 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9_tQU1npEib"
      },
      "outputs": [],
      "source": [
        "sample_incomplete_rows.drop(\"total_bedrooms\", axis=1)       # option 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvzK9G0KpEib"
      },
      "outputs": [],
      "source": [
        "median = housing[\"total_bedrooms\"].median()\n",
        "sample_incomplete_rows[\"total_bedrooms\"].fillna(median, inplace=True) # option 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFMMsG3xpEib"
      },
      "outputs": [],
      "source": [
        "sample_incomplete_rows"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-Learn provides a handy class to take care of missing values \n",
        "\n",
        "**SimpleImputer**.\n",
        "\n"
      ],
      "metadata": {
        "id": "TsqZybypD_ME"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWrUO0jbpEib"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy=\"median\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u_8xkGEpEib"
      },
      "source": [
        "The imputer has simply computed the median of each attribute and stored the result\n",
        "in its statistics_ instance variable. Only the total_bedrooms attribute had missing\n",
        "values, but we cannot be sure that there won’t be any missing values in new data after\n",
        "the system goes live, so it is safer to apply the imputer to all the numerical attributes:\n",
        "\n",
        "Remove the text attribute because median can only be calculated on numerical attributes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSIg-ub6pEib"
      },
      "outputs": [],
      "source": [
        "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
        "# alternatively: housing_num = housing.select_dtypes(include=[np.number])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TZ44idgpEib"
      },
      "outputs": [],
      "source": [
        "imputer.fit(housing_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TE50pwrPpEib"
      },
      "outputs": [],
      "source": [
        "imputer.statistics_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTgZyyH5pEic"
      },
      "source": [
        "Check that this is the same as manually computing the median of each attribute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27GxV1TZpEic"
      },
      "outputs": [],
      "source": [
        "housing_num.median().values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuI1gHuSpEic"
      },
      "source": [
        "**Transform the training set:**\n",
        "\n",
        "Now you can use this “trained” imputer to transform the training set by replacing\n",
        "missing values by the learned medians:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__a6XCAEpEic"
      },
      "outputs": [],
      "source": [
        "X = imputer.transform(housing_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hgGr25HpEic"
      },
      "outputs": [],
      "source": [
        "# The result is a plain NumPy array containing the transformed features. If you want to\n",
        "# put it back into a Pandas DataFrame, it’s simple:\n",
        "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
        "                          index=housing.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzagecB7pEic"
      },
      "outputs": [],
      "source": [
        "\n",
        "housing_tr.loc[sample_incomplete_rows.index.values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31kadb_3pEic"
      },
      "outputs": [],
      "source": [
        "imputer.strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fnOX0fhpEic"
      },
      "source": [
        "## Handling Text and Categorical Attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nEsn_2LpEic"
      },
      "source": [
        "Now let's preprocess the categorical input feature, `ocean_proximity`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Zm9lkUApEic"
      },
      "outputs": [],
      "source": [
        "housing_cat = housing[[\"ocean_proximity\"]]\n",
        "housing_cat.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uth_JNSSpEic"
      },
      "outputs": [],
      "source": [
        "# Most Machine Learning algorithms prefer to work with numbers anyway, \n",
        "# so let’s convert these categories from text to numbers. For this, we can use Scikit-Learn’s OrdinalEncoder class\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
        "housing_cat_encoded[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5uDblaepEic"
      },
      "outputs": [],
      "source": [
        "ordinal_encoder.categories_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One issue with this representation is that ML algorithms will assume that two nearby\n",
        "values are more similar than two distant values. This may be fine in some cases (e.g.,\n",
        "for ordered categories such as “bad”, “average”, “good”, “excellent”), but it is obviously\n",
        "not the case for the ocean_proximity column. \n",
        "\n",
        "This is called **one-hot encoding**, it means\n",
        "only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new attributes are sometimes called **dummy** attributes. \n",
        "\n",
        "Scikit-Learn provides a OneHotEn\n",
        "coder class to convert categorical values into one-hot vectors2"
      ],
      "metadata": {
        "id": "kDaKPR4JGiFq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kp-ZCfZCpEic"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "cat_encoder = OneHotEncoder()\n",
        "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
        "housing_cat_1hot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guKifaB4pEic"
      },
      "source": [
        "By default, the `OneHotEncoder` class returns a sparse matrix, but we can convert it to a dense array if needed by calling the `toarray()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4DHDKKspEic"
      },
      "outputs": [],
      "source": [
        "housing_cat_1hot.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "000kkG_-pEic"
      },
      "source": [
        "Alternatively, you can set `sparse=False` when creating the `OneHotEncoder`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIWzmnrDpEic"
      },
      "outputs": [],
      "source": [
        "cat_encoder = OneHotEncoder(sparse=False)\n",
        "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
        "housing_cat_1hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd-yD-7tpEic"
      },
      "outputs": [],
      "source": [
        "cat_encoder.categories_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrWvjYx3D5OI"
      },
      "source": [
        "## Custom Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz51v7gJD5OI"
      },
      "source": [
        "Let's create a custom transformer to add extra attributes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94ASrFdbD5OI"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# column index\n",
        "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
        "\n",
        "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs\n",
        "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
        "    def fit(self, X, y=None):\n",
        "        return self  # nothing else to do\n",
        "    def transform(self, X):\n",
        "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
        "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
        "        if self.add_bedrooms_per_room:\n",
        "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
        "            return np.c_[X, rooms_per_household, population_per_household,\n",
        "                         bedrooms_per_room]\n",
        "        else:\n",
        "            return np.c_[X, rooms_per_household, population_per_household]\n",
        "\n",
        "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
        "housing_extra_attribs = attr_adder.transform(housing.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APxFbiK5D5OI"
      },
      "source": [
        "Note that I hard coded the indices (3, 4, 5, 6) for concision and clarity in the book, but it would be much cleaner to get them dynamically, like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FX6eAFviD5OI"
      },
      "outputs": [],
      "source": [
        "col_names = \"total_rooms\", \"total_bedrooms\", \"population\", \"households\"\n",
        "rooms_ix, bedrooms_ix, population_ix, households_ix = [\n",
        "    housing.columns.get_loc(c) for c in col_names] # get the column indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoXvbtBFD5OI"
      },
      "source": [
        "Also, `housing_extra_attribs` is a NumPy array, we've lost the column names (unfortunately, that's a problem with Scikit-Learn). To recover a `DataFrame`, you could run this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bN4xouTXD5OI"
      },
      "outputs": [],
      "source": [
        "housing_extra_attribs = pd.DataFrame(\n",
        "    housing_extra_attribs,\n",
        "    columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"],\n",
        "    index=housing.index)\n",
        "housing_extra_attribs.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaDF_3dtD5OI"
      },
      "source": [
        "## Transformation Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfvQpTHmD5OI"
      },
      "source": [
        "Now let's build a pipeline for preprocessing the numerical attributes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95v3DErkD5OJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
        "        ('attribs_adder', CombinedAttributesAdder()),\n",
        "        ('std_scaler', StandardScaler()),\n",
        "    ])\n",
        "\n",
        "housing_num_tr = num_pipeline.fit_transform(housing_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9REZrZ2eD5OJ"
      },
      "outputs": [],
      "source": [
        "housing_num_tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU0kn38QD5OJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "num_attribs = list(housing_num)\n",
        "cat_attribs = [\"ocean_proximity\"]\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "        (\"num\", num_pipeline, num_attribs),\n",
        "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
        "    ])\n",
        "\n",
        "housing_prepared = full_pipeline.fit_transform(housing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWr0TtsXD5OJ"
      },
      "outputs": [],
      "source": [
        "housing_prepared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcfjLSI8D5OJ"
      },
      "outputs": [],
      "source": [
        "housing_prepared.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I2Xt0iEpEie"
      },
      "source": [
        "# Select and Train a Model\n",
        "\n",
        "At last! You framed the problem, you got the data and explored it, you sampled a\n",
        "training set and a test set, and you wrote transformation pipelines to clean up and\n",
        "prepare your data for Machine Learning algorithms automatically. You are now ready\n",
        "to select and train a Machine Learning model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gRqBXYHpEie"
      },
      "source": [
        "## Training and Evaluating on the Training Set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "num_attribs = list(housing_num)\n",
        "cat_attribs = [\"ocean_proximity\"]\n",
        "full_pipeline = ColumnTransformer([\n",
        " (\"num\", num_pipeline, num_attribs),\n",
        " (\"cat\", OneHotEncoder(), cat_attribs),\n",
        " ])\n",
        "housing_prepared = full_pipeline.fit_transform(housing)"
      ],
      "metadata": {
        "id": "aZgBmP469ziN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SD8xXvO6pEie"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(housing_prepared, housing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2I3dB2IpEie"
      },
      "outputs": [],
      "source": [
        "# let's try the full preprocessing pipeline on a few training instances\n",
        "some_data = housing_prepared[15:20]\n",
        "some_labels = housing_labels[15:20]\n",
        "print(\"Predictions:\", lin_reg.predict(some_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw2O4x_hpEie"
      },
      "source": [
        "Compare against the actual values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAv36dx_pEie"
      },
      "outputs": [],
      "source": [
        "print(\"Labels:\", list(some_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7mErL9CpEie"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing_predictions = lin_reg.predict(housing_prepared)\n",
        "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "lin_rmse = np.sqrt(lin_mse)\n",
        "lin_rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhYy0lTTpEie"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "lin_mae = mean_absolute_error(housing_labels, housing_predictions)\n",
        "lin_mae"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an example of a model underfitting\n",
        "the training data. When this happens it can mean that the features do not provide\n",
        "enough information to make good predictions, or that the model is not powerful\n",
        "enough. \n",
        "\n",
        "As we saw, the main ways to fix underfitting are to\n",
        "select a more powerful model, to feed the training algorithm with better features, or\n",
        "to reduce the constraints on the model. This model is not regularized, so this rules\n",
        "out the last option. but first let’s try a more complex model to see how it does. "
      ],
      "metadata": {
        "id": "RK7OP8xUJhal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Regressor\n",
        "\n",
        "Let’s train a DecisionTreeRegressor. This is a powerful model, capable of finding\n",
        "complex nonlinear relationships in the data"
      ],
      "metadata": {
        "id": "4c_3I5cCJ7qC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86nrJgtbpEie"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg = DecisionTreeRegressor(random_state=42)\n",
        "tree_reg.fit(housing_prepared, housing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtW03zDepEie"
      },
      "outputs": [],
      "source": [
        "housing_predictions = tree_reg.predict(housing_prepared)\n",
        "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "tree_rmse = np.sqrt(tree_mse)\n",
        "tree_rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wait, what!? No error at all? Could this model really be absolutely perfect? Of course,\n",
        "it is much more likely that the model has badly overfit the data. How can you be sure?\n",
        "As we saw earlier, you don’t want to touch the test set until you are ready to launch a\n",
        "model you are confident about, so you need to use part of the training set for train‐\n",
        "ing, and part for model validation."
      ],
      "metadata": {
        "id": "G5vwdeTLKPNP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVFTApuzpEie"
      },
      "source": [
        "## Better Evaluation Using Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0n9BfGPGpEie"
      },
      "outputs": [],
      "source": [
        "#Scikit-Learn’s K-fold cross-validation feature. \n",
        "# It randomly splits the training set into 10 distinct subsets called folds, then it\n",
        "# trains and evaluates the Decision Tree model 10 times, picking a different fold for\n",
        "# evaluation every time and training on the other 9 folds. The result is an array con‐\n",
        "# taining the 10 evaluation scores:\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
        "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
        "tree_rmse_scores = np.sqrt(-scores)\n",
        "\n",
        "# Scikit-Learn’s cross-validation features expect a utility function\n",
        "# (greater is better) rather than a cost function (lower is better), so\n",
        "# the scoring function is actually the opposite of the MSE (i.e., a negative value), \n",
        "#which is why the preceding code computes -scores before calculating the square root."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IUaaYjopEie"
      },
      "outputs": [],
      "source": [
        "def display_scores(scores):\n",
        "    print(\"Scores:\", scores)\n",
        "    print(\"Mean:\", scores.mean())\n",
        "    print(\"Standard deviation:\", scores.std())\n",
        "\n",
        "display_scores(tree_rmse_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DecisionTreeRegressor seems to perform worse than the Linear Regression model! \n",
        "\n",
        "Notice that cross-validation allows\n",
        "you to get not only an estimate of the performance of your model, but also a measure\n",
        "of how precise this estimate is (i.e., its standard deviation)."
      ],
      "metadata": {
        "id": "1jLJLSe_Lbw-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqR0krTQpEif"
      },
      "outputs": [],
      "source": [
        "# Let’s compute the same scores for the Linear Regression model just to be sure:\n",
        "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n",
        "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
        "lin_rmse_scores = np.sqrt(-lin_scores)\n",
        "display_scores(lin_rmse_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest Regressor\n",
        "\n",
        "Random Forests work by training many Decision Trees on random subsets of\n",
        "the features, then averaging out their predictions. \n",
        "\n",
        "Building a model on top of many\n",
        "other models is called Ensemble Learning, and it is often a great way to push ML algorithms even further. "
      ],
      "metadata": {
        "id": "ju3qSEI5MAPz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WITctQkZpEif"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "forest_reg.fit(housing_prepared, housing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lslg7FDnpEif"
      },
      "outputs": [],
      "source": [
        "housing_predictions = forest_reg.predict(housing_prepared)\n",
        "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "forest_rmse = np.sqrt(forest_mse)\n",
        "forest_rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEjXkDalpEif"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
        "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
        "forest_rmse_scores = np.sqrt(-forest_scores)\n",
        "display_scores(forest_rmse_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dO_f-hypEif"
      },
      "outputs": [],
      "source": [
        "scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
        "pd.Series(np.sqrt(-scores)).describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow, this is much better: Random Forests look very promising. However, note that\n",
        "the score on the training set is still much lower than on the validation sets, meaning\n",
        "that the model is still overfitting the training set. \n",
        "\n",
        "Possible solutions for overfitting are\n",
        "to simplify the model, constrain it (i.e., regularize it), or get a lot more training data.\n",
        "\n",
        "\n",
        "However, before you dive much deeper in Random Forests, you should try out many\n",
        "other models from various categories of Machine Learning algorithms (several Sup‐\n",
        "port Vector Machines with different kernels, possibly a neural network, etc.), without\n",
        "spending too much time tweaking the hyperparameters. The goal is to shortlist a few\n",
        "(two to five) promising models."
      ],
      "metadata": {
        "id": "0pKrH6UlNcUW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQkwwLLIpEif"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "svm_reg = SVR(kernel=\"linear\")\n",
        "svm_reg.fit(housing_prepared, housing_labels)\n",
        "housing_predictions = svm_reg.predict(housing_prepared)\n",
        "svm_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "svm_rmse = np.sqrt(svm_mse)\n",
        "svm_rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mBHPg0mpEif"
      },
      "source": [
        "# Fine-Tune Your Model\n",
        "\n",
        "Let’s assume that you now have a shortlist of promising models. You now need to\n",
        "fine-tune them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2s9rYCEpEif"
      },
      "source": [
        "## Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuMzk_MKpEif"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = [\n",
        "    # try 12 (3×4) combinations of hyperparameters\n",
        "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
        "    # then try 6 (2×3) combinations with bootstrap set as False\n",
        "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
        "  ]\n",
        "\n",
        "forest_reg = RandomForestRegressor(random_state=42)\n",
        "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
        "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
        "                           scoring='neg_mean_squared_error',\n",
        "                           return_train_score=True)\n",
        "grid_search.fit(housing_prepared, housing_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQf1Pw3ZpEif"
      },
      "source": [
        "The best hyperparameter combination found:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O72MzkBpEif"
      },
      "outputs": [],
      "source": [
        "grid_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UiiQygLpEif"
      },
      "outputs": [],
      "source": [
        "grid_search.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m12j6QqpEif"
      },
      "source": [
        "Let's look at the score of each hyperparameter combination tested during the grid search:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZQQLxVfpEif"
      },
      "outputs": [],
      "source": [
        "cvres = grid_search.cv_results_\n",
        "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
        "    print(np.sqrt(-mean_score), params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoNzbjDepEif"
      },
      "source": [
        "## Randomized Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OzpOaU-pEif"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "param_distribs = {\n",
        "        'n_estimators': randint(low=1, high=200),\n",
        "        'max_features': randint(low=1, high=8),\n",
        "    }\n",
        "\n",
        "forest_reg = RandomForestRegressor(random_state=42)\n",
        "rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n",
        "                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
        "rnd_search.fit(housing_prepared, housing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBolICN5pEig"
      },
      "outputs": [],
      "source": [
        "cvres = rnd_search.cv_results_\n",
        "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
        "    print(np.sqrt(-mean_score), params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uv4XPJjpEig"
      },
      "source": [
        "## Analyze the Best Models and Their Errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWoNlL8JpEig"
      },
      "outputs": [],
      "source": [
        "feature_importances = grid_search.best_estimator_.feature_importances_\n",
        "feature_importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rooA81vupEig"
      },
      "outputs": [],
      "source": [
        "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
        "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
        "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
        "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
        "sorted(zip(feature_importances, attributes), reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tv79rJ6pEig"
      },
      "source": [
        "## Evaluate Your System on the Test Set\n",
        "\n",
        "After tweaking your models for a while, you eventually have a system that performs\n",
        "sufficiently well. Now is the time to evaluate the final model on the test set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyGfsrUMpEig"
      },
      "outputs": [],
      "source": [
        "final_model = grid_search.best_estimator_\n",
        "\n",
        "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
        "y_test = strat_test_set[\"median_house_value\"].copy()\n",
        "\n",
        "X_test_prepared = full_pipeline.transform(X_test) # call transform(), not fit_transform(), you do not want to fit the test set!\n",
        "final_predictions = final_model.predict(X_test_prepared)\n",
        "\n",
        "final_mse = mean_squared_error(y_test, final_predictions)\n",
        "final_rmse = np.sqrt(final_mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XB_of3GWpEig"
      },
      "outputs": [],
      "source": [
        "final_rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh3X0ubhpEig"
      },
      "source": [
        "We can compute a 95% confidence interval for the test RMSE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ox1jmpEPpEig"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "\n",
        "confidence = 0.95\n",
        "squared_errors = (final_predictions - y_test) ** 2\n",
        "np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n",
        "                         loc=squared_errors.mean(),\n",
        "                         scale=stats.sem(squared_errors)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now comes the project prelaunch phase: you need to present your solution (highlighting what you have learned, what worked and what did not, what assumptions\n",
        "were made, and what your system’s limitations are), document everything, and create\n",
        "nice presentations with clear visualizations and easy-to-remember statements (e.g.,\n",
        "“the median income is the number one predictor of housing prices”)\n",
        "\n",
        "In this California housing example, the final performance of the system is not better than the\n",
        "experts’, but it may still be a good idea to launch it, especially if this frees up some\n",
        "time for the experts so they can work on more interesting and productive tasks."
      ],
      "metadata": {
        "id": "zqEj413_R3_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Launch, Monitor, and Maintain Your System\n"
      ],
      "metadata": {
        "id": "mox5VxlKUuKg"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "nav_menu": {
      "height": "279px",
      "width": "309px"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "02_end_to_end_machine_learning_project.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}