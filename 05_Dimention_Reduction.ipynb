{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOP7JG496LbbaP6e2M7zbSP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pejmanrasti/From_Shallow_to_Deep/blob/main/05_Dimention_Reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We demonstrates here the impact of dimensionality reduction using **Component Analysis (PCA)** on both classification and regression tasks.  It provides a comparative analysis of model performance (accuracy for classification, mean squared error for regression) and computational efficiency (time taken for training and prediction) with and without PCA.\n",
        "\n",
        "We use uses two popular datasets: the MNIST handwritten digit dataset for classification and the California housing dataset for regression.  For each dataset, it trains a model (Support Vector Classifier for MNIST and Linear Regression for California housing) with and without applying PCA beforehand. PCA transforms the high-dimensional data into a lower-dimensional space while retaining most of the important variance, potentially improving performance and reducing computational costs."
      ],
      "metadata": {
        "id": "So7fYIUD3STQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load MNIST dataset from Keras\n",
        "print(\"Loading the MNIST dataset from Keras...\")\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Flatten the 28x28 images into vectors of size 784\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# Scenario 1: Classification without dimensionality reduction\n",
        "print(\"Running classification without dimensionality reduction...\")\n",
        "start_time = time.time()\n",
        "clf_no_reduction = SVC(random_state=42)\n",
        "clf_no_reduction.fit(X_train, y_train)\n",
        "y_pred_no_reduction = clf_no_reduction.predict(X_test)\n",
        "time_no_reduction = time.time() - start_time\n",
        "accuracy_no_reduction = accuracy_score(y_test, y_pred_no_reduction)\n",
        "\n",
        "# Scenario 2: Classification with PCA for dimensionality reduction\n",
        "print(\"Applying PCA for dimensionality reduction...\")\n",
        "pca = PCA(n_components=50)  # Reduce to 50 dimensions\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "print(\"Running classification with dimensionality reduction...\")\n",
        "start_time = time.time()\n",
        "clf_with_reduction = SVC(random_state=42)\n",
        "clf_with_reduction.fit(X_train_pca, y_train)\n",
        "y_pred_with_reduction = clf_with_reduction.predict(X_test_pca)\n",
        "time_with_reduction = time.time() - start_time\n",
        "accuracy_with_reduction = accuracy_score(y_test, y_pred_with_reduction)\n",
        "\n",
        "# Display the results\n",
        "print(\"\\nResults:\")\n",
        "print(f\"Accuracy without PCA: {accuracy_no_reduction:.2f}, Time taken: {time_no_reduction:.2f} seconds\")\n",
        "print(f\"Accuracy with PCA: {accuracy_with_reduction:.2f}, Time taken: {time_with_reduction:.2f} seconds\")\n",
        "\n",
        "# Visualize the results\n",
        "labels = [\"Without PCA\", \"With PCA\"]\n",
        "accuracy = [accuracy_no_reduction, accuracy_with_reduction]\n",
        "time_taken = [time_no_reduction, time_with_reduction]\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.barplot(x=labels, y=accuracy)\n",
        "plt.title(\"Accuracy Comparison\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.ylim(0.8, 1)\n",
        "\n",
        "# Time plot\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.barplot(x=labels, y=time_taken)\n",
        "plt.title(\"Time Comparison\")\n",
        "plt.ylabel(\"Time (seconds)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualizing PCA effect on data (first two components)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap=\"viridis\", s=5)\n",
        "plt.colorbar(label=\"Digit Label\")\n",
        "plt.title(\"PCA Reduced Data Visualization (First 2 Components)\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Swn3ZxpV2Xsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the California housing dataset\n",
        "print(\"Loading the California housing dataset...\")\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Scenario 1: Regression without dimensionality reduction\n",
        "print(\"Running regression without dimensionality reduction...\")\n",
        "start_time = time.time()\n",
        "reg_no_reduction = LinearRegression()\n",
        "reg_no_reduction.fit(X_train, y_train)\n",
        "y_pred_no_reduction = reg_no_reduction.predict(X_test)\n",
        "time_no_reduction = time.time() - start_time\n",
        "mse_no_reduction = mean_squared_error(y_test, y_pred_no_reduction)\n",
        "\n",
        "# Scenario 2: Regression with PCA for dimensionality reduction\n",
        "print(\"Applying PCA for dimensionality reduction...\")\n",
        "pca = PCA(n_components=3)  # Reduce to 3 dimensions\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "print(\"Running regression with dimensionality reduction...\")\n",
        "start_time = time.time()\n",
        "reg_with_reduction = LinearRegression()\n",
        "reg_with_reduction.fit(X_train_pca, y_train)\n",
        "y_pred_with_reduction = reg_with_reduction.predict(X_test_pca)\n",
        "time_with_reduction = time.time() - start_time\n",
        "mse_with_reduction = mean_squared_error(y_test, y_pred_with_reduction)\n",
        "\n",
        "# Display the results\n",
        "print(\"\\nResults:\")\n",
        "print(f\"Mean Squared Error without PCA: {mse_no_reduction:.2f}, Time taken: {time_no_reduction:.2f} seconds\")\n",
        "print(f\"Mean Squared Error with PCA: {mse_with_reduction:.2f}, Time taken: {time_with_reduction:.2f} seconds\")\n",
        "\n",
        "# Visualize the results\n",
        "labels = [\"Without PCA\", \"With PCA\"]\n",
        "mse = [mse_no_reduction, mse_with_reduction]\n",
        "time_taken = [time_no_reduction, time_with_reduction]\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# MSE plot\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.barplot(x=labels, y=mse)\n",
        "plt.title(\"Mean Squared Error Comparison\")\n",
        "plt.ylabel(\"MSE\")\n",
        "\n",
        "# Time plot\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.barplot(x=labels, y=time_taken)\n",
        "plt.title(\"Time Comparison\")\n",
        "plt.ylabel(\"Time (seconds)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "obWAe4O72gJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 1:  Varying PCA Components\n",
        "Modify the n_components parameter in PCA for both the classification and regression examples.\n",
        "\n",
        "Experiment with different values (e.g., 10, 20, 100, 700) or Use ratio for the MNIST dataset and observe how the accuracy and time taken change.\n",
        "Analyze the impact of different numbers of components on the model's performance and time efficiency.\n",
        "\n",
        "# Exercise 2:  Alternative Dimensionality Reduction Technique (t-SNE)\n",
        "Replace PCA with t-SNE (t-distributed Stochastic Neighbor Embedding) for dimensionality reduction in the MNIST classification example.\n",
        "\n",
        "Compare the results (accuracy, time) of t-SNE with PCA. Analyze the differences in their performance, focusing on how well each preserves local neighborhood structures.\n",
        "\n",
        "**Note:** t-SNE is often used for visualization, but you can experiment with it as a dimensionality reduction method.  It is computationally more expensive.\n",
        "\n",
        "# Exercise 3:  Applying LDA to MNIST\n",
        "Apply Linear Discriminant Analysis (LDA) to reduce the dimensionality of the MNIST dataset before classification.\n",
        "\n",
        "Compare the accuracy and computational time with the previous methods (PCA and t-SNE).  LDA is a supervised method; it uses class labels during dimensionality reduction.\n",
        "\n",
        "Does LDA's use of class labels improve classification accuracy, and how much slower is it?\n",
        "\n",
        "LDA is often better for classification tasks because it maximizes class separability.\n",
        "\n",
        "# Exercise 4: Feature Scaling and Dimensionality Reduction\n",
        "Experiment with different feature scaling methods (e.g., MinMaxScaler, StandardScaler) before applying PCA or other dimensionality reduction techniques.\n",
        "Evaluate how feature scaling affects the performance of dimensionality reduction methods (accuracy and training time).  For example:\n",
        "\n",
        "**from sklearn.preprocessing import StandardScaler**\n",
        "\n",
        "**scaler = StandardScaler()**\n",
        "\n",
        "**X_train_scaled = scaler.fit_transform(X_train)**\n",
        "\n",
        "**X_test_scaled = scaler.transform(X_test)**\n",
        "\n",
        "# Exercise 5:  Kernel PCA\n",
        "Use KernelPCA instead of standard PCA. Experiment with different kernels (linear, rbf, poly)and observe their impact on MNIST classification. Compare results against those from regular PCA.\n",
        "\n",
        "Kernel PCA can capture non-linear relationships in the data.\n"
      ],
      "metadata": {
        "id": "rXNqHKBr1Wns"
      }
    }
  ]
}